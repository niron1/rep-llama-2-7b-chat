#!/usr/bin/env python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
CACHE_DIR = 'cache'

tokenizer = AutoTokenizer.from_pretrained(
    "daryl149/Llama-2-7b-chat-hf",
    use_cache=CACHE_DIR,
)
model = AutoModelForCausalLM.from_pretrained(
    "daryl149/Llama-2-7b-chat-hf",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
    use_cache=CACHE_DIR,
)
