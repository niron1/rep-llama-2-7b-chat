#!/usr/bin/env python
import os
import shutil
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
CACHE_DIR = 'cache'

if os.path.exists(CACHE_DIR):
    shutil.rmtree(CACHE_DIR)

os.makedirs(CACHE_DIR)

tokenizer = AutoTokenizer.from_pretrained(
    "daryl149/Llama-2-7b-chat-hf",
    use_cache=CACHE_DIR,
    cache_dir=CACHE_DIR
)
model = AutoModelForCausalLM.from_pretrained(
    "daryl149/Llama-2-7b-chat-hf",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    use_cache=CACHE_DIR,
    cache_dir=CACHE_DIR
)
